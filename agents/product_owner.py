from langchain_core.prompts import ChatPromptTemplate
from core.llm_factory import get_chat_model
from core.state import ProjectState

llm = get_chat_model(temperature=0.3)

PO_SYSTEM_PROMPT = """
JesteÅ› Product Ownerem. StwÃ³rz Backlog na podstawie pomysÅ‚u uÅ¼ytkownika.
Zawrzyj: Cel biznesowy, User Stories, Kryteria Akceptacji.
Nie pisz kodu.
"""

def product_owner_node(state: ProjectState) -> ProjectState:
    print("\nğŸ© Product Owner: StartujÄ™...")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", PO_SYSTEM_PROMPT),
        ("user", state["user_request"])
    ])
    
    chain = prompt | llm
    
    print(f"ğŸ© Product Owner: WysyÅ‚am zapytanie do modelu... (To moÅ¼e chwilÄ™ potrwaÄ‡)")
    response = chain.invoke({})
    print("ğŸ© Product Owner: OtrzymaÅ‚em odpowiedÅº!")
    
    return {
        "requirements": response.content,
        "logs": ["Product Owner stworzyÅ‚ backlog."]
    }
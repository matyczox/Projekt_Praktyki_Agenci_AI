from langchain_core.prompts import ChatPromptTemplate
from core.llm_factory import get_chat_model
from core.state import ProjectState

# 1. Pobieramy model "Reasoning" (ten mƒÖdrzejszy, Llama 3.3 70b)
llm = get_chat_model(temperature=0.3)

# 2. Definiujemy "Osobowo≈õƒá" agenta (System Prompt)
# To tutaj dzieje siƒô magia - ka≈ºemy mu byƒá konkretnym.
PO_SYSTEM_PROMPT = """
Jeste≈õ do≈õwiadczonym Product Ownerem w zespole Agile.
Tw√≥j cel: Przeanalizowaƒá pomys≈Ç u≈ºytkownika i stworzyƒá profesjonalnƒÖ specyfikacjƒô (Backlog).

Twoja odpowied≈∫ MUSI zawieraƒá:
1. **Cel Biznesowy**: Jedno zdanie, co budujemy.
2. **User Stories**: Lista funkcjonalno≈õci w formacie "Jako u≈ºytkownik chcƒô..., aby...".
3. **Kryteria Akceptacji**: Co musi dzia≈Çaƒá, ≈ºeby uznaƒá zadanie za sko≈Ñczone.

NIE PISZ KODU. Skup siƒô na logice biznesowej i wymaganiach.
Pisz zwiƒô≈∫le, punktuj, u≈ºywaj Markdown.
"""

def product_owner_node(state: ProjectState) -> ProjectState:
    """
    Funkcja, kt√≥ra jest 'Wƒôz≈Çem' (Node) w naszym grafie.
    Przyjmuje stan, my≈õli, i zwraca zaktualizowany stan.
    """
    print("\nüé© Product Owner: Analizujƒô wymagania...")
    
    # Tworzymy prompt: System Prompt + Pro≈õba u≈ºytkownika ze stanu
    prompt = ChatPromptTemplate.from_messages([
        ("system", PO_SYSTEM_PROMPT),
        ("user", state["user_request"])
    ])
    
    # ≈ÅƒÖczymy Prompt z Modelem
    chain = prompt | llm
    
    # Uruchamiamy (Invoke)
    response = chain.invoke({})
    
    print("‚úÖ Product Owner: Specyfikacja gotowa.")
    
    # Zwracamy TYLKO to, co siƒô zmieni≈Ço (wymagania i logi)
    return {
        "requirements": response.content,
        "logs": ["Product Owner stworzy≈Ç backlog."]
    }
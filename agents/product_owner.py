from langchain_core.prompts import ChatPromptTemplate
from core.llm_factory import get_chat_model
from core.state import ProjectState
import os

# UÅ¼ywamy Llamy 70B (Reasoning)
llm = get_chat_model(os.getenv("MODEL_REASONING", "llama3.3:70b"), temperature=0.2)

SYSTEM_PROMPT = """
JesteÅ› Technical Leadem (AI Copilot).
Twoim zadaniem jest analiza pomysÅ‚u uÅ¼ytkownika i stworzenie konkretnej SPECYFIKACJI TECHNICZNEJ.

ZASADY:
1. Olej korpo-gadkÄ™ i "cele biznesowe".
2. Skup siÄ™ na TECHNIKALIACH: funkcje, logika, biblioteki.
3. Pisz w punktach. KrÃ³tko, zwiÄ™Åºle, technicznie.
4. To ma byÄ‡ instrukcja dla Architekta i Programisty.
5. Nie generuj kodu ani struktur plikÃ³w.
"""

def product_owner_node(state: ProjectState) -> ProjectState:
    print("\nðŸ§  Tech Lead: AnalizujÄ™ zadanie...")
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        ("user", f"Zadanie: {state['user_request']}")
    ])
    response = (prompt | llm).invoke({})
    return {
        "requirements": response.content,
        "logs": ["Tech Lead przygotowaÅ‚ specyfikacjÄ™."]
    }
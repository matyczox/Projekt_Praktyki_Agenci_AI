from langchain_core.prompts import ChatPromptTemplate
from core.llm_factory import get_chat_model
from core.state import ProjectState

llm = get_chat_model(temperature=0.1)

ARCHITECT_SYSTEM_PROMPT = """
JesteÅ› Architektem. Zaprojektuj stack technologiczny i listÄ™ plikÃ³w.
Projekt musi byÄ‡ w Pythonie.
"""

def architect_node(state: ProjectState) -> ProjectState:
    print("\nğŸ“ Architekt: ProjektujÄ™...")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", ARCHITECT_SYSTEM_PROMPT),
        ("user", f"Wymagania:\n{state.get('requirements')}")
    ])
    
    print("ğŸ“ Architekt: GenerujÄ™ plan...")
    response = (prompt | llm).invoke({})
    
    return {
        "tech_stack": response.content,
        "logs": ["Architekt stworzyÅ‚ strukturÄ™."]
    }
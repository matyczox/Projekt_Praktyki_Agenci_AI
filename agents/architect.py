from langchain_core.prompts import ChatPromptTemplate
from core.llm_factory import get_chat_model
from core.state import ProjectState
import os

llm = get_chat_model(os.getenv("MODEL_REASONING", "llama3.3:70b"), temperature=0.1)

ARCHITECT_SYSTEM_PROMPT = """
Jeste G贸wnym Architektem System贸w IT.
Zaprojektuj struktur plik贸w dla projektu.

ZASADY KRYTYCZNE:
1. Zaplanuj sensowny podzia na pliki (logika, widoki, config).
2. KRTKO opisz przeznaczenie ka偶dego pliku.
3. ABSOLUTNY ZAKAZ generowania kodu implementacyjnego.
4. W licie JSON uwzgldnij TYLKO pliki tekstowe (.py, .js, .html, .css, .md, .txt).
5. NIE WPISUJ do JSONa obrazk贸w (.png) ani d藕wik贸w (.wav).
6. Na samym kocu odpowiedzi MUSISZ wygenerowa blok JSON z list plik贸w.

PRZYKAD FORMATU KOCOWEGO:
...opis...
```json
[
  "main.py",
  "game_logic.py",
  "requirements.txt"
]
```
"""

def architect_node(state: ProjectState) -> ProjectState:
    print("\n Architekt: Projektuj struktur...")
    prompt = ChatPromptTemplate.from_messages([
        ("system", ARCHITECT_SYSTEM_PROMPT),
        ("user", f"Wymagania:\n{state.get('requirements')}")
    ])
    response = (prompt | llm).invoke({})
    return {
        "tech_stack": response.content,
        "logs": ["Architekt zaprojektowa struktur."]
    }
from langchain_core.prompts import ChatPromptTemplate
from core.llm_factory import get_chat_model
from core.state import ProjectState

llm = get_chat_model(temperature=0.1)

# --- ZMIANA: DodaliÅ›my instrukcje o README i requirements ---
ARCHITECT_SYSTEM_PROMPT = """
JesteÅ› GÅ‚Ã³wnym Architektem Systemu.
Twoim zadaniem jest zaprojektowaÄ‡ rozwiÄ…zanie techniczne na podstawie wymagaÅ„.

WYMAGANIA KRYTYCZNE:
1. Stack technologiczny: Python.
2. ZAWSZE uwzglÄ™dnij plik 'requirements.txt' z listÄ… bibliotek (np. pygame, pandas).
3. ZAWSZE uwzglÄ™dnij plik 'README.md' z instrukcjÄ…, jak zainstalowaÄ‡ zaleÅ¼noÅ›ci i uruchomiÄ‡ program.
4. Rozbij kod na sensowne moduÅ‚y, jeÅ›li projekt jest duÅ¼y.

Twoja odpowiedÅº to plan dla Developera.
"""

def architect_node(state: ProjectState) -> ProjectState:
    print("\nğŸ“ Architekt: ProjektujÄ™ strukturÄ™ systemu...")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", ARCHITECT_SYSTEM_PROMPT),
        ("user", f"Wymagania od PO:\n{state.get('requirements')}")
    ])
    
    print("ğŸ“ Architekt: GenerujÄ™ plan techniczny...")
    response = (prompt | llm).invoke({})
    
    return {
        "tech_stack": response.content,
        "logs": ["Architekt stworzyÅ‚ strukturÄ™ z dokumentacjÄ…."]
    }
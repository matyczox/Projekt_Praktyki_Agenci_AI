from langchain_core.prompts import ChatPromptTemplate
from core.llm_factory import get_chat_model
from core.state import ProjectState

# Architekt musi byÄ‡ mÄ…dry, wiÄ™c teÅ¼ uÅ¼ywamy modelu Reasoning (Llama 3.3)
llm = get_chat_model(temperature=0.1)

ARCHITECT_SYSTEM_PROMPT = """
JesteÅ› GÅ‚Ã³wnym Architektem Systemu (Solution Architect).
Twoim zadaniem jest zaprojektowaÄ‡ rozwiÄ…zanie techniczne na podstawie wymagaÅ„ biznesowych.

Twoja odpowiedÅº MUSI zawieraÄ‡:
1. **Stack Technologiczny**: JÄ™zyk, frameworki, baza danych.
2. **Struktura Projektu**: Lista plikÃ³w i folderÃ³w, ktÃ³re naleÅ¼y utworzyÄ‡.
3. **Plan Implementacji**: KrÃ³tka instrukcja dla programisty, od czego zaczÄ…Ä‡.

PAMIÄ˜TAJ:
- Projekt musi byÄ‡ w Pythonie.
- StrukturÄ™ plikÃ³w przedstaw jako listÄ™ wypunktowanÄ….
- Nie pisz jeszcze peÅ‚nego kodu, tylko nazwy plikÃ³w i co majÄ… robiÄ‡.
"""

def architect_node(state: ProjectState) -> ProjectState:
    print("\nğŸ“ Architekt: ProjektujÄ™ strukturÄ™ systemu...")
    
    # Pobieramy wymagania ze stanu (to co wypluÅ‚ wczeÅ›niej PO)
    requirements = state.get("requirements", "Brak wymagaÅ„.")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", ARCHITECT_SYSTEM_PROMPT),
        ("user", f"Oto wymagania od Product Ownera:\n\n{requirements}")
    ])
    
    chain = prompt | llm
    response = chain.invoke({})
    
    print("âœ… Architekt: Plan techniczny gotowy.")
    
    # Zapisujemy plan do stanu, Å¼eby Developer mÃ³gÅ‚ go przeczytaÄ‡
    return {
        "tech_stack": response.content,
        "logs": ["Architekt stworzyÅ‚ strukturÄ™ plikÃ³w."]
    }